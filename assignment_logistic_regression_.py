# -*- coding: utf-8 -*-
"""Assignment Logistic_Regression .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XG_5AgZfwZjVrmbPwvETOAhM1nic5oCx
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

data_train=pd.read_csv("Titanic_train.csv")

data_train.head()

data_test=pd.read_csv("Titanic_test.csv")

data_test.head()

from google.colab import drive
drive.mount('/content/drive')

data_train.info()

data_train.isnull().sum()

age_mean=data_train['Age'].mean()

data_train['Age'].fillna(age_mean,inplace=True)

cabin_mode=data_train['Cabin'].mode()

cabin_mode

# data_train['Cabin'].fillna(cabin_mode,inplace=True)

data_train.isnull().sum()

# data_train['Cabin'].fillna(0)

# data_train['Cabin'].drop

data_train

data_train.describe()  #here we can see the statistics of data

data_train.isnull().sum()

# data_train.drop(['Cabin'],axis=1,inplace=True)  #here we drop unecessary columns

data_train

data_train.dtypes

em=data_train["Embarked"].mode()

data_train['Embarked'].fillna(em)

#we done with eda process now we do visulization
# Histograms for numerical features
data_train.hist(bins=20, figsize=(15, 10), color='skyblue', edgecolor='black')
plt.suptitle("Histograms of Features", fontsize=16)
plt.tight_layout()
plt.show()

# # Pair plot for relationships
# sns.pairplot(data_train, hue='Survived')
# plt.suptitle("Pair Plot of Features", y=1.02, fontsize=16)
# plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(data_train.select_dtypes(include=['int','float']).corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap", fontsize=16)
plt.show()

"""### Data Preprocessing
Handle Missing Values and Encode Categorical Variables
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler   #here we import important packages

#split data into x and y
X=data_train.drop(columns=['Survived'])
y=data_train['Survived']

X

# Standardize numerical features
scaler = StandardScaler()
X = scaler.fit_transform(X.select_dtypes(include='int'))

X

# Split into training and testing sets (80% training, 20% testing)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=45)

"""###  Model Building
Train Logistic Regression Model
"""

from sklearn.linear_model import LogisticRegression

model=LogisticRegression()

model

model.fit(X_train,y_train)

"""### Model Evaluation
Evaluate Model Performance
"""

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, ConfusionMatrixDisplay
)

y_pred_train=model.predict(X_train)  #here we do prediction on seen data

y_pred_train

# Predictions
y_pred_test = model.predict(X_test)
y_pred_prob = model.predict_proba(X_test)[:, 1]

y_pred_test
y_pred_prob

# Metrics see the model perfomance using the matrics
accuracy = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)
roc_auc = roc_auc_score(y_test, y_pred_prob)

accuracy,precision,recall,f1,roc_auc



accuracy = accuracy_score(y_train,y_pred_train)
precision = precision_score(y_train, y_pred_train)
recall = recall_score(y_train, y_pred_train)
f1 = f1_score(y_train, y_pred_train)
# roc_auc = roc_auc_score(y_test, y_pred_prob)

accuracy,precision,recall,f1

#we can see the accuracy of traning and testing data model give accuracy by looking accuracy we can say model is underfiting

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})", color="darkorange")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend()
plt.show()

"""### Interpretation of Coefficients"""

# Coefficients of the logistic regression model
# coefficients = pd.DataFrame({
#     "Feature": data_train.drop(columns=['Survived']).columns,  # Replace 'target' with your target column
#     "Coefficient": model.coef_[0]
# })
# coefficients["Importance"] = coefficients["Coefficient"].abs()
# coefficients.sort_values(by="Importance", ascending=False, inplace=True)

# print("Feature Importance:")
# print(coefficients)

# Logistic Regression Coefficients

print("\nLogistic Regression Coefficients:")
for feature, coef in zip(data_train.columns, model.coef_[0]):
    print(f"{feature}: {coef:.3f}")

"""###  Visualizations
Enhancements Suggested:

Add visualizations such as:

Boxplots to visualize outliers in continuous features.

KDE (Kernel Density Estimation) plots for understanding the distribution of features.

Code Snippet for Boxplot and KDE Plot:
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Boxplot for Fare to visualize outliers
plt.figure(figsize=(8, 5))
sns.boxplot(data=data_train, x='Fare')
plt.title("Fare Boxplot")
plt.show()

# Outlier Treatment
# Apply robust methods to handle outliers.

Q1 = data_train['Fare'].quantile(0.25)
Q3 = data_train['Fare'].quantile(0.75)
IQR = Q3 - Q1

# Outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Replace outliers or remove them
data_train['Fare'] = data_train['Fare'].clip(lower_bound, upper_bound)

# Multicollinearity Check (VIF Analysis)
# Variance Inflation Factor (VIF) helps detect multicollinearity.

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Select numerical features for VIF
X = data_train[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].dropna()
X = add_constant(X)

# Calculate VIF for each feature
vif_df = pd.DataFrame()
vif_df["Feature"] = X.columns
vif_df["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
print(vif_df)

# Deployment

# ! pip install streamlit

"""### Interview Questions


1. What is the Difference Between Precision and Recall?

Precision: The proportion of true positive predictions out of all positive predictions.

Recall (Sensitivity): The proportion of true positive predictions out of all actual positive cases.

Trade-off: Higher precision reduces false positives, while higher recall reduces false negatives.


2. What is Cross-Validation, and Why Is It Important in Binary Classification?

Definition: Cross-validation splits the dataset into multiple training and validation sets to evaluate the model's performance.
Why Important?
Reduces overfitting by testing on unseen data.
Provides a more reliable estimate of model performance.
Ensures the model generalizes well.


What is Logistic Regression? Logistic regression is a supervised machine learning algorithm used for binary classification tasks. It models the probability of a categorical outcome based on one or more predictor variables using the logistic function.

Why do we check for multicollinearity in regression models? Multicollinearity can distort the coefficient estimates, leading to unreliable predictions. Detecting and removing multicollinear features ensures better model interpretability and performance.


"""

# jupyter nbconvert --to script your_notebook.ipynb

import pickle

# Save the trained model as a pickle file
with open('logistic_regression_model.pkl', 'wb') as file:
    pickle.dump(model, file)

import streamlit as st
import pickle
import pandas as pd

# Load the model
with open('logistic_regression_model.pkl', 'rb') as file:
    model = pickle.load(file)

st.title("Titanic Survival Prediction")

# Input form
Pclass = st.number_input("Passenger Class (1, 2, 3)")
Age = st.number_input("Age")
SibSp = st.number_input("Number of Siblings/Spouses Aboard")
Parch = st.number_input("Number of Parents/Children Aboard")
Fare = st.number_input("Passenger Fare")

if st.button("Predict"):
    input_data = pd.DataFrame([[Pclass, Age, SibSp, Parch, Fare]],
                               columns=['Pclass', 'Age', 'SibSp', 'Parch', 'Fare'])
    prediction = model.predict(input_data)[0]
    result = "Survived" if prediction == 1 else "Did not survive"
    st.write(f"Prediction: {result}")

# ! pip install streamlit

streamlit run app.py









